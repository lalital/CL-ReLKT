{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import MicroTokenizer\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from rank_bm25 import BM25Okapi, BM25Plus\n",
    "from pythainlp.tokenize import word_tokenize \n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_encode(contexts,language):\n",
    "    if language == 'zh':\n",
    "        context_split = [*map(MicroTokenizer.cut,contexts)]\n",
    "    elif language == 'th':\n",
    "        context_split = [word_tokenize(x, engine=\"newmm\") for x in contexts]\n",
    "    else:\n",
    "        context_split = [x.split(' ') for x in contexts]\n",
    "        \n",
    "    context_encoded = BM25Okapi(context_split)\n",
    "    return context_encoded\n",
    "\n",
    "def bm25_scoring(question,context_encoded,language):\n",
    "    if language == 'zh':\n",
    "        tokenized_query = MicroTokenizer.cut(question)\n",
    "    elif language == 'th':\n",
    "        tokenized_query = word_tokenize(question, engine=\"newmm\")\n",
    "    else:\n",
    "        tokenized_query = question.split(' ')\n",
    "        \n",
    "    ranking = context_encoded.get_scores(tokenized_query)\n",
    "    return np.argsort(ranking)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_para(queries,query_id,paras,para_id,bm25_para_encoded,lan,para_df_f):\n",
    "    triplet_datasets = []\n",
    "    for idx,q in tqdm(enumerate(queries)):\n",
    "        index_sorted = bm25_scoring(q,bm25_para_encoded,lan)\n",
    "        para_answer = para_df_f[para_df_f['para_id'] == query_id[idx]].para.values.tolist()\n",
    "        count = 0\n",
    "        for index in index_sorted[top_start:]:\n",
    "            para_neg = paras[index]\n",
    "            if count == top_end:\n",
    "                break\n",
    "            if para_neg not in para_answer:\n",
    "                for p_a in para_answer:\n",
    "                    triplet_datasets.append([q,p_a,para_neg,query_id[idx],para_id[index]])\n",
    "                count+=1\n",
    "    return triplet_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_para_contrastive(queries,query_id,paras,para_id,bm25_para_encoded,lan,para_df_f):\n",
    "    triplet_datasets = []\n",
    "    for idx,q in tqdm(enumerate(queries)):\n",
    "        index_sorted = bm25_scoring(q,bm25_para_encoded,lan)\n",
    "        para_answer = para_df_f[para_df_f['para_id'] == query_id[idx]].para.values.tolist()\n",
    "        count = 0\n",
    "        for index in index_sorted[top_start:]:\n",
    "            para_neg = paras[index]\n",
    "            if count == top_end:\n",
    "                break\n",
    "            if para_neg not in para_answer:\n",
    "                for p_a in para_answer:\n",
    "                    triplet_datasets.append([q,p_a,para_neg,query_id[idx],para_id[index]])\n",
    "                count+=1\n",
    "    return triplet_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_para_aligned(queries,query_id_doc,query_id_para,paras,para_id,bm25_para_encoded,lan,para_df_f):\n",
    "    triplet_datasets = []\n",
    "    for idx,q in tqdm(enumerate(queries)):\n",
    "        index_sorted = bm25_scoring(q,bm25_para_encoded,lan)\n",
    "        para_answers = para_df_f[para_df_f['doc_id'] == query_id_doc[idx]].para.values.tolist()\n",
    "        para_item = para_df_f[para_df_f['para_id'] == query_id_para[idx]].para.values.tolist()\n",
    "        if len(para_item) > 1:\n",
    "            raise Exception(f'Error at query: {para_item}')\n",
    "        count = 0\n",
    "        for index in index_sorted[top_start:]:\n",
    "            para_neg = paras[index]\n",
    "            if count == top_end:\n",
    "                break\n",
    "            if para_neg not in para_answers:\n",
    "                for p_a in para_item:\n",
    "                    triplet_datasets.append([q,p_a,para_neg])\n",
    "                count+=1\n",
    "    return triplet_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_para_doc(queries,query_id,paras,para_id,bm25_para_encoded,lan):\n",
    "    triplet_datasets = []\n",
    "    for idx,q in tqdm(enumerate(queries)):\n",
    "        index_sorted = bm25_scoring(q,bm25_para_encoded,lan)\n",
    "        count = 0\n",
    "        for index in index_sorted[:]:\n",
    "            para_neg = paras[index]\n",
    "            if count == top_end:\n",
    "                break\n",
    "            if para_neg not in paras[idx]:\n",
    "                triplet_datasets.append([q,paras[idx],para_neg])\n",
    "                count+=1\n",
    "    return triplet_datasets\n",
    "\n",
    "def get_score_doc(queries,query_id,docs,docs_id,bm25_doc_encoded,lan):\n",
    "    triplet_datasets = []\n",
    "    for idx,q in enumerate(queries):\n",
    "        index_sorted = bm25_scoring(q,bm25_doc_encoded,lan)\n",
    "        doc_answer_idx = docs_id.index(query_id[idx])\n",
    "        doc_answer = docs[doc_answer_idx]\n",
    "        count = 0\n",
    "        for index in index_sorted[:]:\n",
    "            if count == top_end:\n",
    "                break\n",
    "            if index != doc_answer_idx:\n",
    "                triplet_datasets.append([q,doc_answer,docs[index]])\n",
    "                count+=1\n",
    "    return triplet_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_splited(doc,doc_id):\n",
    "    splited = []\n",
    "    data_frame = []\n",
    "    for idx,d in enumerate(doc):\n",
    "        for d_p in d.split('\\n'):\n",
    "            if d_p != '':\n",
    "                data_frame.append([doc_id[idx],d_p])\n",
    "                splited.append(d_p)\n",
    "    data_frame = pd.DataFrame(data_frame ,columns =['para_id', 'para'])\n",
    "    return splited,data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "def clean_text(text):\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ru', 'bn', 'fi', 'ja', 'ko', 'te']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = 'train'\n",
    "q_lan = ['en','ar','de','es','hi','vi','zh','ru','bn','fi','ja','ko','te','fr','th','el','ro','tr']\n",
    "corpus = 'XORQA'\n",
    "\n",
    "path = f'data_preprocess/{corpus}/{mode}/'\n",
    "top_start = 1  \n",
    "top_end = 16\n",
    "\n",
    "df_question = {}\n",
    "df_paragraph = {}\n",
    "df_doc = {}\n",
    "df_question_og = {}\n",
    "lan_now = []\n",
    "\n",
    "for lan in q_lan:\n",
    "    try:\n",
    "        question_temp = pd.read_csv(f'data_preprocess/{corpus}/{mode}/{corpus.lower()}_question_en-{lan}_en.csv')\n",
    "        question_temp_2 = pd.read_csv(f'data_preprocess/{corpus}/{mode}/{corpus.lower()}_question_en-{lan}.csv')\n",
    "        doc_temp = pd.read_csv(f'data_preprocess/{corpus}/{mode}/{corpus.lower()}_doc_en-{lan}.csv')\n",
    "        question_temp = question_temp.dropna()\n",
    "\n",
    "        df_question.update({\n",
    "            f'en-{lan}': question_temp\n",
    "        })\n",
    "        df_question_og.update({\n",
    "            f'en-{lan}': question_temp_2\n",
    "        })\n",
    "        \n",
    "#         df_paragraph.update({\n",
    "#             f'en-{lan}':pd.read_csv(f'data_preprocess/{corpus}/{mode}/{corpus.lower()}_para_en-{lan}.csv')\n",
    "#         })\n",
    "\n",
    "        df_doc.update({\n",
    "            f'en-{lan}': doc_temp\n",
    "        })\n",
    "        lan_now.append(lan)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "q_lan = lan_now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lan:1/12\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'paragraph_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'paragraph_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e7fd45e7b7c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Lan:{idx+1}/{len(q_lan)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mquestion_id_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_question\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'en-{lan}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mquestion_id_para\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_question\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'en-{lan}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraph_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_question\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'en-{lan}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2891\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2892\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2893\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'paragraph_id'"
     ]
    }
   ],
   "source": [
    "top_start = 1  \n",
    "top_end = 3\n",
    "context_mode = 'para_aligned'\n",
    "for idx,lan in enumerate(q_lan[:1]):\n",
    "    print(f'Lan:{idx+1}/{len(q_lan)}')\n",
    "    question_id_doc = df_question[f'en-{lan}']['doc_id'].to_list()\n",
    "    question_id_para = df_question[f'en-{lan}']['paragraph_id'].to_list()\n",
    "    questions = df_question[f'en-{lan}']['question'].to_list()\n",
    "\n",
    "    para_raw = df_paragraph[f'en-{lan}']['para'].to_list()\n",
    "\n",
    "    para_encoded = bm25_encode(para_raw,lan)\n",
    "\n",
    "    trip_data = get_score_para_aligned(questions,question_id_doc,question_id_para,para_raw,df_paragraph[f'en-{lan}']['doc_id'].to_list(),para_encoded,lan,df_paragraph[f'en-{lan}'])\n",
    "    df = pd.DataFrame(trip_data,columns=['question','anchor','negative'])\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    if not os.path.exists(f'{path}/triplet'):\n",
    "        os.makedirs(f'{path}/triplet')\n",
    "    df.to_csv(f'{path}/triplet/triplet_en-{lan}_top{top_start}-{top_end}_{context_mode}_new_no_same_doc.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lan:1/4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069c8ccae3fd48b38ecc8a515f6f2ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lan:2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a4b0cae6284627a465e8e95a882f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lan:3/4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee0494388514f0bad7e75dac4fd135e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lan:4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade908fde52849ec8d992ffd132d1a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "top_start = 1  \n",
    "top_end = 3\n",
    "context_mode = 'para'\n",
    "\n",
    "for idx,lan in enumerate(q_lan[:]):\n",
    "    print(f'Lan:{idx+1}/{len(q_lan)}')\n",
    "    q_no_dup = df_question[f'en-{lan}'].drop_duplicates(subset=['question'])\n",
    "    \n",
    "    question_id = q_no_dup['doc_id'].to_list()\n",
    "    questions = q_no_dup['question'].to_list()\n",
    "    questions = list(map(clean_text,questions))\n",
    "    \n",
    "    doc_context_id = df_doc[f'en-{lan}']['doc_id'].to_list()\n",
    "    doc_context_raw = df_doc[f'en-{lan}']['doc'].to_list()\n",
    "    doc_context_raw = list(map(clean_text,doc_context_raw))\n",
    "    \n",
    "    para_split,para_df = paragraph_splited(doc_context_raw,doc_context_id)\n",
    "\n",
    "    para_encoded = bm25_encode(para_split,lan)\n",
    "\n",
    "    trip_data = get_score_para(questions,question_id,para_split,para_df['para_id'].to_list(),para_encoded,lan,para_df)\n",
    "    df = pd.DataFrame(trip_data,columns=['anchor','positive','negative','a_p_id','n_id'])\n",
    "    ne_df = df_question_og[f\"en-{lan}\"]\n",
    "    df_concat = pd.merge(df,fi_df,left_on='a_p_id',right_on='doc_id')\n",
    "    df_concat = df_concat.drop(columns=['anchor'])\n",
    "    df_concat = df_concat.rename(columns={\"question\": \"anchor\"})\n",
    "    \n",
    "    df = df_concat[['anchor','positive','negative']]\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    if not os.path.exists(f'{path}/triplet'):\n",
    "        os.makedirs(f'{path}/triplet')\n",
    "    df.to_csv(f'{path}/triplet/triplet_en-{lan}_top{top_start}-{top_end}_{context_mode}_new.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
