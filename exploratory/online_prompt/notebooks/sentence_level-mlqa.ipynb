{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-14T09:32:56.564473Z",
     "iopub.status.idle": "2022-06-14T09:32:56.564869Z",
     "shell.execute_reply": "2022-06-14T09:32:56.564718Z",
     "shell.execute_reply.started": "2022-06-14T09:32:56.564700Z"
    },
    "tags": []
   },
   "source": [
    "## Sentence-level online prompty mining: MLQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:21:59.013823Z",
     "iopub.status.busy": "2022-06-14T10:21:59.013543Z",
     "iopub.status.idle": "2022-06-14T10:22:04.990727Z",
     "shell.execute_reply": "2022-06-14T10:22:04.990273Z",
     "shell.execute_reply.started": "2022-06-14T10:21:59.013762Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import re\n",
    "import os, sys\n",
    "import json\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "\n",
    "from exploring_sentence_level import (\n",
    "    load_model,\n",
    "    mine_prompt_gt,  \n",
    "    segment_sentence,\n",
    "    run_online_prompt_mining\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Download dataset\n",
    "\n",
    "```bash\n",
    "cd ../scripts\n",
    "bash ./download_mlqa.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:22:04.992342Z",
     "iopub.status.busy": "2022-06-14T10:22:04.992149Z",
     "iopub.status.idle": "2022-06-14T10:22:05.677192Z",
     "shell.execute_reply": "2022-06-14T10:22:05.676507Z",
     "shell.execute_reply.started": "2022-06-14T10:22:04.992324Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en', 'ar', 'de', 'es', 'hi', 'vi', 'zh']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLQA_BASE_DIR = '../data/mlqa/MLQA_V1/'\n",
    "\n",
    "mlqa_xx = {}\n",
    "MLQA_LANGS = ['en', 'ar', 'de', 'es', 'hi', 'vi', 'zh']\n",
    "for lang in MLQA_LANGS:\n",
    "    mlqa_xx[f'{lang}_val'] = json.load(open(os.path.join(MLQA_BASE_DIR, 'dev', f'dev-context-en-question-{lang}.json'), 'r'))['data'],\n",
    "    mlqa_xx[f'{lang}_test'] = json.load(open(os.path.join(MLQA_BASE_DIR, 'test', f'test-context-en-question-{lang}.json'), 'r'))['data'],\n",
    "MLQA_LANGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:22:05.678417Z",
     "iopub.status.busy": "2022-06-14T10:22:05.678031Z",
     "iopub.status.idle": "2022-06-14T10:22:05.690162Z",
     "shell.execute_reply": "2022-06-14T10:22:05.689430Z",
     "shell.execute_reply.started": "2022-06-14T10:22:05.678364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2389"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlqa_xx['ar_test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:22:05.691413Z",
     "iopub.status.busy": "2022-06-14T10:22:05.691163Z",
     "iopub.status.idle": "2022-06-14T10:22:05.695418Z",
     "shell.execute_reply": "2022-06-14T10:22:05.694844Z",
     "shell.execute_reply.started": "2022-06-14T10:22:05.691388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_squad_answer_str(context, qas):\n",
    "    context_qa_pairs = []\n",
    "    for qa in qas:\n",
    "        question = qa['question']\n",
    "        answer = qa['answers'][0]['text']\n",
    "        answer_start = qa['answers'][0]['answer_start']\n",
    "        context_qa_pairs.append((context, question, answer, answer_start))\n",
    "    return context_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:22:05.698133Z",
     "iopub.status.busy": "2022-06-14T10:22:05.697843Z",
     "iopub.status.idle": "2022-06-14T10:22:30.697724Z",
     "shell.execute_reply": "2022-06-14T10:22:30.697182Z",
     "shell.execute_reply.started": "2022-06-14T10:22:05.698100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlqa_xx_dataset = defaultdict(lambda: {'val':[], 'test': []})\n",
    "\n",
    "mlqa_sentences = defaultdict(lambda: [])\n",
    "\n",
    "\n",
    "for lang in MLQA_LANGS:\n",
    "    global_paragraph_id = 0\n",
    "    global_sentence_id = 0\n",
    "    for split_name in ['val', 'test']:\n",
    "        for i, item in enumerate(mlqa_xx[f'{lang}_{split_name}'][0]):\n",
    "\n",
    "            title = item['title']\n",
    "            paragraphs = item['paragraphs']\n",
    "\n",
    "            for j, paragraph in enumerate(paragraphs):\n",
    "\n",
    "                context = paragraph['context']\n",
    "                context_qa_pairs = get_squad_answer_str(context=context, qas=paragraph['qas'])\n",
    "                segmented_context = segment_sentence(context)\n",
    "                segmented_context_ids = []\n",
    "                \n",
    "                for sentence_id in range(len(segmented_context)):\n",
    "                    mlqa_sentences[lang].append((title, global_paragraph_id, global_sentence_id, segmented_context[sentence_id], split_name))\n",
    "                    segmented_context_ids.append(global_sentence_id)\n",
    "                    global_sentence_id += 1\n",
    "                \n",
    "                for context_qa_pair in context_qa_pairs:\n",
    "                    context, question, answer, answer_start = context_qa_pair\n",
    "                    gt_sentence, gt_sentence_idx = mine_prompt_gt(context_qa_pair)\n",
    "                    gt_sentence_global_idx = segmented_context_ids[gt_sentence_idx]\n",
    "                    \n",
    "                    qa_item = {\n",
    "                         'question': question,\n",
    "                         'context': context,\n",
    "                         'segmented_context': segment_sentence(context),\n",
    "                         'segmented_context_ids': segmented_context_ids,\n",
    "                         'answer': answer,\n",
    "                         'answer_start': answer_start,\n",
    "                         'split_name': split_name,\n",
    "                         'gt_sentence': gt_sentence,\n",
    "                         'gt_sentence_idx': gt_sentence_global_idx,\n",
    "\n",
    "                    }\n",
    "                    mlqa_xx_dataset[lang][split_name].append(qa_item)\n",
    "                global_paragraph_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:22:30.698832Z",
     "iopub.status.busy": "2022-06-14T10:22:30.698482Z",
     "iopub.status.idle": "2022-06-14T10:22:30.702788Z",
     "shell.execute_reply": "2022-06-14T10:22:30.701798Z",
     "shell.execute_reply.started": "2022-06-14T10:22:30.698811Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 5335)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlqa_xx_dataset['ar']['val']), \\\n",
    "len(mlqa_xx_dataset['ar']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:22:30.704271Z",
     "iopub.status.busy": "2022-06-14T10:22:30.703847Z",
     "iopub.status.idle": "2022-06-14T10:22:30.709506Z",
     "shell.execute_reply": "2022-06-14T10:22:30.708852Z",
     "shell.execute_reply.started": "2022-06-14T10:22:30.704247Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who analyzed the biopsies?',\n",
       " 'context': 'In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"',\n",
       " 'segmented_context': ['In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency.',\n",
       "  'Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom.',\n",
       "  'Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat.',\n",
       "  'The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza.',\n",
       "  'The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials).',\n",
       "  'They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors.',\n",
       "  'Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"'],\n",
       " 'segmented_context_ids': [6317, 6318, 6319, 6320, 6321, 6322, 6323],\n",
       " 'answer': 'Rutgers University biochemists',\n",
       " 'answer_start': 457,\n",
       " 'split_name': 'test',\n",
       " 'gt_sentence': 'Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat.',\n",
       " 'gt_sentence_idx': 6319}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlqa_xx_dataset['en']['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write (All language) segmented sentences into separated csv file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['en', 'ar', 'de', 'es', 'hi', 'vi', 'zh'], 7, 34917)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mlqa_sentences.keys()), len(mlqa_sentences), len(mlqa_sentences['ar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in list(mlqa_sentences.keys()):\n",
    "    if len(mlqa_sentences[lang]) > 0:\n",
    "        mlqa_sentences_df = pd.DataFrame.from_dict(mlqa_sentences[lang])\n",
    "        mlqa_sentences_df.columns=['doc_title', 'paragraph_id', 'sentence_id', 'sentence', 'split_name']\n",
    "\n",
    "        mlqa_sentences_df.to_csv(f'./question-sentences-pairs/mlqa/mlqa_sentence-en_for-question-{lang}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write (All languages) question-sentence pairs into separated csv file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['en', 'ar', 'de', 'es', 'hi', 'vi', 'zh']),\n",
       " dict_keys(['val', 'test']))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlqa_xx_dataset.keys(), mlqa_xx_dataset['zh'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '这项工作最初的计划名称是什么？',\n",
       " 'context': 'Poe planned for years to produce his own journal The Penn (later renamed The Stylus), but he died before it could be produced. He died in Baltimore on October 7, 1849, at age 40; the cause of his death is unknown and has been variously attributed to alcohol, \"brain congestion\", cholera, drugs, heart disease, rabies, suicide, tuberculosis, and other causes.Poe and his works influenced literature around the world, as well as specialized fields such as cosmology and cryptography. He and his work appear throughout popular culture in literature, music, films, and television. A number of his homes are dedicated museums today. The Mystery Writers of America present an annual award known as the Edgar Award for distinguished work in the mystery genre.',\n",
       " 'segmented_context': ['Poe planned for years to produce his own journal The Penn (later renamed The Stylus), but he died before it could be produced.',\n",
       "  'He died in Baltimore on October 7, 1849, at age 40; the cause of his death is unknown and has been variously attributed to alcohol, \"brain congestion\", cholera, drugs, heart disease, rabies, suicide, tuberculosis, and other causes.Poe and his works influenced literature around the world, as well as specialized fields such as cosmology and cryptography.',\n",
       "  'He and his work appear throughout popular culture in literature, music, films, and television.',\n",
       "  'A number of his homes are dedicated museums today.',\n",
       "  'The Mystery Writers of America present an annual award known as the Edgar Award for distinguished work in the mystery genre.'],\n",
       " 'segmented_context_ids': [0, 1, 2, 3, 4],\n",
       " 'answer': 'The Penn',\n",
       " 'answer_start': 49,\n",
       " 'split_name': 'val',\n",
       " 'gt_sentence': 'Poe planned for years to produce his own journal The Penn (later renamed The Stylus), but he died before it could be produced.',\n",
       " 'gt_sentence_idx': 0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlqa_xx_dataset['zh']['val'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question_lang in list(mlqa_xx_dataset.keys()):\n",
    "    \n",
    "    for split_name in list(mlqa_xx_dataset[question_lang].keys()):\n",
    "\n",
    "        if len(mlqa_xx_dataset[question_lang][split_name]) > 0:\n",
    "            mlqa_question_sentence_pairs_df = pd.DataFrame.from_dict(list(map(lambda x: (x['question'], x['gt_sentence_idx']), mlqa_xx_dataset[question_lang][split_name])))\n",
    "            mlqa_question_sentence_pairs_df.columns = ['question' , 'gt_sentence_idx']\n",
    "\n",
    "            mlqa_question_sentence_pairs_df.to_csv(f'./question-sentences-pairs/mlqa/mlqa-{split_name}_question-{question_lang}_sentence-en.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute question-sentence similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Load mUSE_small (v3) model (as a baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T09:25:42.613822Z",
     "iopub.status.busy": "2022-06-14T09:25:42.613593Z",
     "iopub.status.idle": "2022-06-14T09:32:56.562260Z",
     "shell.execute_reply": "2022-06-14T09:32:56.561136Z",
     "shell.execute_reply.started": "2022-06-14T09:25:42.613798Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "muse_small_v3_model = load_model('https://tfhub.dev/google/universal-sentence-encoder-multilingual/3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Load teacher models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-14T09:32:56.562723Z",
     "iopub.status.idle": "2022-06-14T09:32:56.562967Z",
     "shell.execute_reply": "2022-06-14T09:32:56.562842Z",
     "shell.execute_reply.started": "2022-06-14T09:32:56.562829Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "XQUAD_TEACHER_DIR = '../../../../CL-ReLKT_store/models/XQUAD/teacher_model/'\n",
    "MLQA_TEACHER_DIR = '../../../../CL-ReLKT_store/models/MLQA/teacher_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xquad_teacher_model = load_model(XQUAD_TEACHER_DIR)\n",
    "mlqa_teacher_model = load_model(MLQA_TEACHER_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Load student models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "XQUAD_STUDENT_SUPPORTED_LANGS_DIR = '../../../../CL-ReLKT_store/models/XQUAD/student_best_supported_languages/'\n",
    "XQUAD_STUDENT_UNSUPPORTED_LANGS_DIR = '../../../../CL-ReLKT_store/models/XQUAD/student_best_unsupported_languages/'\n",
    "\n",
    "XORQA_STUDENT_SUPPORTED_LANGS_DIR = '../../../../CL-ReLKT_store/models/XORQA/student_best_supported_languages/'\n",
    "XORQA_STUDENT_UNSUPPORTED_LANGS_DIR = '../../../../CL-ReLKT_store/models/XORQA/student_best_unsupported_languages/'\n",
    "\n",
    "MLQA_STUDENT_SUPPORTED_LANGS_DIR = '../../../../CL-ReLKT_store/models/MLQA/student_best_supported_languages/'\n",
    "MLQA_STUDENT_UNSUPPORTED_LANGS_DIR = '../../../../CL-ReLKT_store/models/MLQA/student_best_unsupported_languages/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xquad_student_supported_langs_model = load_model(XQUAD_STUDENT_SUPPORTED_LANGS_DIR)\n",
    "xorqa_student_supported_langs_model = load_model(XORQA_STUDENT_SUPPORTED_LANGS_DIR)\n",
    "mlqa_student_supported_langs_model = load_model(MLQA_STUDENT_SUPPORTED_LANGS_DIR)\n",
    "\n",
    "xquad_student_unsupported_langs_model = load_model(XQUAD_STUDENT_UNSUPPORTED_LANGS_DIR)\n",
    "xorqa_student_unsupported_langs_model = load_model(XORQA_STUDENT_UNSUPPORTED_LANGS_DIR)\n",
    "mlqa_student_unsupported_langs_model = load_model(MLQA_STUDENT_UNSUPPORTED_LANGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MAPPING = {\n",
    "  # mUSE_small\n",
    "  'model-muse_small_v3': muse_small_v3_model,\n",
    "  # teacher    \n",
    "  'model-xquad_teacher': xquad_teacher_model,\n",
    "  'model-mlqa_teacher': mlqa_teacher_model,\n",
    "  # student\n",
    "  'model-xquad_student_supported_langs': xquad_student_supported_langs_model,\n",
    "  'model-xorqa_student_supported_langs': xorqa_student_supported_langs_model,\n",
    "  'model-mlqa_student_supported_langs': mlqa_student_supported_langs_model,\n",
    "  'model-xquad_student_unsupported_langs': xquad_student_unsupported_langs_model,\n",
    "  'model-xorqa_student_unsupported_langs': xorqa_student_unsupported_langs_model,\n",
    "  'model-mlqa_student_unsupported_langs': mlqa_student_unsupported_langs_model,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:22:30.713341Z",
     "iopub.status.busy": "2022-06-14T10:22:30.713175Z",
     "iopub.status.idle": "2022-06-14T10:22:30.720107Z",
     "shell.execute_reply": "2022-06-14T10:22:30.718877Z",
     "shell.execute_reply.started": "2022-06-14T10:22:30.713320Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_MAPPING = {}\n",
    "for lang in list(MLQA_LANGS):\n",
    "    DATASET_MAPPING[f'dataset-mlqa_{lang.strip()}_val'] = mlqa_xx_dataset[lang]['val']\n",
    "    DATASET_MAPPING[f'dataset-mlqa_{lang.strip()}_test'] = mlqa_xx_dataset[lang]['test']\n",
    "    \n",
    "print(DATASET_MAPPING.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Run inference and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function `run_online_prompt_mining` iterates over question-answer-passage triplets $(q_i, a_i, p_i)$ and compute \n",
    "the cosine similarity scores between question $q_i$ and segmented setences $s^i_j \\textrm{ where } p_i = ( s^i_0, \\ldots , s^i_{|p_i| - 1} )$ , and rank each quesiton-sentence pair by similairy score. Then, it evaluate the sentence-level precision@k.  Note: There is only 1 groundtruth sentence (i.e. the sentence where the answer span is a part of). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "dataset_prefix: dataset-mlqa_en_val\n",
      "\n",
      " - model_prefix: model-muse_small_v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [02:24<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.7125\n",
      "\t - precision_at_k:\n",
      "{1: 0.7125435540069687,\n",
      " 2: 0.8614982578397212,\n",
      " 3: 0.9242160278745645,\n",
      " 4: 0.9512195121951219,\n",
      " 5: 0.9634146341463414,\n",
      " 6: 0.9747386759581882,\n",
      " 7: 0.9825783972125436,\n",
      " 8: 0.9878048780487805,\n",
      " 9: 0.9895470383275261,\n",
      " 10: 0.9912891986062717}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " - model_prefix: model-xquad_teacher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [02:05<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.7247\n",
      "\t - precision_at_k:\n",
      "{1: 0.7247386759581882,\n",
      " 2: 0.8641114982578397,\n",
      " 3: 0.9259581881533101,\n",
      " 4: 0.9494773519163763,\n",
      " 5: 0.9651567944250871,\n",
      " 6: 0.975609756097561,\n",
      " 7: 0.985191637630662,\n",
      " 8: 0.9878048780487805,\n",
      " 9: 0.9895470383275261,\n",
      " 10: 0.9921602787456446}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " - model_prefix: model-mlqa_teacher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [02:21<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.7265\n",
      "\t - precision_at_k:\n",
      "{1: 0.7264808362369338,\n",
      " 2: 0.8693379790940766,\n",
      " 3: 0.9233449477351916,\n",
      " 4: 0.9494773519163763,\n",
      " 5: 0.9651567944250871,\n",
      " 6: 0.9747386759581882,\n",
      " 7: 0.9817073170731707,\n",
      " 8: 0.990418118466899,\n",
      " 9: 0.9912891986062717,\n",
      " 10: 0.9930313588850174}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " - model_prefix: model-xquad_student_supported_langs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [02:22<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.5166\n",
      "\t - precision_at_k:\n",
      "{1: 0.5165505226480837,\n",
      " 2: 0.7125435540069687,\n",
      " 3: 0.823170731707317,\n",
      " 4: 0.8797909407665505,\n",
      " 5: 0.9172473867595818,\n",
      " 6: 0.9416376306620209,\n",
      " 7: 0.9581881533101045,\n",
      " 8: 0.9686411149825784,\n",
      " 9: 0.9747386759581882,\n",
      " 10: 0.980836236933798}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " - model_prefix: model-xorqa_student_supported_langs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [02:19<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.6916\n",
      "\t - precision_at_k:\n",
      "{1: 0.6916376306620209,\n",
      " 2: 0.8336236933797909,\n",
      " 3: 0.9033101045296167,\n",
      " 4: 0.9346689895470384,\n",
      " 5: 0.9512195121951219,\n",
      " 6: 0.9686411149825784,\n",
      " 7: 0.9799651567944251,\n",
      " 8: 0.9843205574912892,\n",
      " 9: 0.9895470383275261,\n",
      " 10: 0.9921602787456446}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " - model_prefix: model-mlqa_student_supported_langs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [02:22<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.7265\n",
      "\t - precision_at_k:\n",
      "{1: 0.7264808362369338,\n",
      " 2: 0.8667247386759582,\n",
      " 3: 0.921602787456446,\n",
      " 4: 0.9477351916376306,\n",
      " 5: 0.9634146341463414,\n",
      " 6: 0.9738675958188153,\n",
      " 7: 0.9825783972125436,\n",
      " 8: 0.990418118466899,\n",
      " 9: 0.990418118466899,\n",
      " 10: 0.9930313588850174}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " - model_prefix: model-xquad_student_unsupported_langs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [02:26<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.4007\n",
      "\t - precision_at_k:\n",
      "{1: 0.40069686411149824,\n",
      " 2: 0.5984320557491289,\n",
      " 3: 0.7412891986062717,\n",
      " 4: 0.8310104529616724,\n",
      " 5: 0.8806620209059234,\n",
      " 6: 0.9111498257839721,\n",
      " 7: 0.936411149825784,\n",
      " 8: 0.9503484320557491,\n",
      " 9: 0.9625435540069687,\n",
      " 10: 0.9686411149825784}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " - model_prefix: model-xorqa_student_unsupported_langs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [02:23<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.6838\n",
      "\t - precision_at_k:\n",
      "{1: 0.6837979094076655,\n",
      " 2: 0.8153310104529616,\n",
      " 3: 0.89198606271777,\n",
      " 4: 0.936411149825784,\n",
      " 5: 0.9529616724738676,\n",
      " 6: 0.9660278745644599,\n",
      " 7: 0.980836236933798,\n",
      " 8: 0.9843205574912892,\n",
      " 9: 0.9886759581881533,\n",
      " 10: 0.990418118466899}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " - model_prefix: model-mlqa_student_unsupported_langs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [02:23<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.7073\n",
      "\t - precision_at_k:\n",
      "{1: 0.7073170731707317,\n",
      " 2: 0.8466898954703833,\n",
      " 3: 0.912020905923345,\n",
      " 4: 0.9442508710801394,\n",
      " 5: 0.9590592334494773,\n",
      " 6: 0.9703832752613241,\n",
      " 7: 0.980836236933798,\n",
      " 8: 0.9869337979094077,\n",
      " 9: 0.9886759581881533,\n",
      " 10: 0.9921602787456446}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "dataset_prefix: dataset-mlqa_en_test\n",
      "\n",
      " - model_prefix: model-muse_small_v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11590/11590 [23:08<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.6963\n",
      "\t - precision_at_k:\n",
      "{1: 0.6962899050905953,\n",
      " 2: 0.844521138912856,\n",
      " 3: 0.911130284728214,\n",
      " 4: 0.9438308886971527,\n",
      " 5: 0.9614322691975842,\n",
      " 6: 0.9720448662640208,\n",
      " 7: 0.9811044003451251,\n",
      " 8: 0.9855910267471959,\n",
      " 9: 0.9883520276100086,\n",
      " 10: 0.9907679033649698}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " - model_prefix: model-xquad_teacher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11590/11590 [23:19<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.7035\n",
      "\t - precision_at_k:\n",
      "{1: 0.7034512510785159,\n",
      " 2: 0.8440897325280414,\n",
      " 3: 0.9097497842968076,\n",
      " 4: 0.9440034512510785,\n",
      " 5: 0.962381363244176,\n",
      " 6: 0.9715271786022434,\n",
      " 7: 0.9805004314063848,\n",
      " 8: 0.9858498705780846,\n",
      " 9: 0.9885245901639345,\n",
      " 10: 0.991458153580673}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " - model_prefix: model-mlqa_teacher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11590/11590 [23:30<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEvaluation result:\n",
      "\t - Accuracy: 0.7112\n",
      "\t - precision_at_k:\n",
      "{1: 0.7112165660051769,\n",
      " 2: 0.8484037963761863,\n",
      " 3: 0.9101811906816221,\n",
      " 4: 0.9446937014667817,\n",
      " 5: 0.9602243313201035,\n",
      " 6: 0.9715271786022434,\n",
      " 7: 0.980327868852459,\n",
      " 8: 0.9855910267471959,\n",
      " 9: 0.9891285591026747,\n",
      " 10: 0.9904227782571182}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " - model_prefix: model-xquad_student_supported_langs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 710/11590 [01:26<23:32,  7.70it/s]"
     ]
    }
   ],
   "source": [
    "results = defaultdict(lambda : defaultdict())\n",
    "\n",
    "for dataset_prefix, dataset in DATASET_MAPPING.items():\n",
    "    print(f'\\n\\ndataset_prefix: {dataset_prefix}')\n",
    "    for model_prefix, model in MODEL_MAPPING.items():\n",
    "        \n",
    "        print(f'\\n - model_prefix: {model_prefix}')\n",
    "        prefix = f'{dataset_prefix}+{model_prefix}'\n",
    "        _result = run_online_prompt_mining(dataset,\n",
    "                             prefix=f'{dataset_prefix}_{model_prefix}',\n",
    "                             model=model)\n",
    "\n",
    "\n",
    "        results[dataset_prefix][model_prefix] = _result\n",
    "        print('--'*50)\n",
    "    print('\\n')    \n",
    "    print('=='*50)\n",
    "    print('\\n')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Write result as JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results, open('./eval_results.dataset_name-mlqa.json', 'w'), ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert evaluation results to a pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = json.load(open('./eval_results.dataset_name-mlqa.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results.keys()), len(list(results.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_objs = []\n",
    "for dataset_name, result_model_group in results.items():\n",
    "    for model_name, (metric, raw_result) in result_model_group.items():\n",
    "        top1, precision_at_k = metric\n",
    "        \n",
    "        result_objs.append({\n",
    "            'dataset_name': dataset_name,\n",
    "            'model_name': model_name,\n",
    "            'precision_at_1': top1,\n",
    "            'precision_at_2': precision_at_k['2'],\n",
    "            'precision_at_3': precision_at_k['6'],\n",
    "            'precision_at_4': precision_at_k['4'],\n",
    "            'precision_at_5': precision_at_k['5'],\n",
    "            'precision_at_10': precision_at_k['10'],\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame.from_dict(result_objs)\n",
    "df.to_csv('./eval_results.dataset_name-mlqa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
