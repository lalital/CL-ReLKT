{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-14T09:32:56.564473Z",
     "iopub.status.idle": "2022-06-14T09:32:56.564869Z",
     "shell.execute_reply": "2022-06-14T09:32:56.564718Z",
     "shell.execute_reply.started": "2022-06-14T09:32:56.564700Z"
    },
    "tags": []
   },
   "source": [
    "## Sentence-level online prompty mining: XORQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:24:30.769548Z",
     "iopub.status.busy": "2022-06-14T10:24:30.769276Z",
     "iopub.status.idle": "2022-06-14T10:24:30.773345Z",
     "shell.execute_reply": "2022-06-14T10:24:30.772778Z",
     "shell.execute_reply.started": "2022-06-14T10:24:30.769517Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import re\n",
    "import os, sys\n",
    "import json\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import jsonlines\n",
    "\n",
    "from exploring_sentence_level import (\n",
    "    load_model,\n",
    "    mine_prompt_gt,\n",
    "    segment_sentence,\n",
    "    run_online_prompt_mining\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Download dataset\n",
    "\n",
    "```bash\n",
    "cd ../scripts\n",
    "bash ./download_xorqa.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:24:39.821027Z",
     "iopub.status.busy": "2022-06-14T10:24:39.820738Z",
     "iopub.status.idle": "2022-06-14T10:24:40.025856Z",
     "shell.execute_reply": "2022-06-14T10:24:40.025050Z",
     "shell.execute_reply.started": "2022-06-14T10:24:39.820997Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "XORQA_BASE_DIR = '../data/xorqa/en/tydi_xor_gp/'\n",
    "xorqa_xx = {\n",
    "    'train': json.load(open(os.path.join(XORQA_BASE_DIR, 'gp_squad_train_data.json'), 'r'))['data'],\n",
    "      'val': json.load(open(os.path.join(XORQA_BASE_DIR, 'gp_squad_dev_data.json'), 'r'))['data'],   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:24:44.979844Z",
     "iopub.status.busy": "2022-06-14T10:24:44.979400Z",
     "iopub.status.idle": "2022-06-14T10:24:44.985010Z",
     "shell.execute_reply": "2022-06-14T10:24:44.984377Z",
     "shell.execute_reply.started": "2022-06-14T10:24:44.979814Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'title:Vamsy_parentSection:Introduction_sectionName:Career._sectionIndex:2',\n",
       " 'paragraphs': [{'context': 'He has published a short stories compilation called \"Maa Pasalapudi Kathalu\". Besides that compilation, Vamsy has written a wide variety of short stories since 1974 when he was 18 years old. His major works include \"Mahallo kokila\", \"Manchupallaki\", \"Aa Naati Vaana Chinukulu\", \"Venditera Kathalu\" (original scripts of \"Sankarabharanam\" and \"Anveshana\"), \"Vennela Bomma\", \"Gokulam lo Radha\", \"Ravvala konda\", \"Sree seetarama lanchi service Rajahmundry\", \"Manyam rani\", \"Rangularatnam\". He has penned around 150 short stories published in swathi weekly under title \"Maa Diguwa Godavari Kathalu\" For his contributions to the art of story telling with a native approach through his books he was bestowed with \"Sripada Puraskhaaram\" at Rajamundry on 17 April 2011.',\n",
       "   'qas': [{'question': 'మా పసలపూడి కథలు పుస్తకమును ఎవరు రచించారు?',\n",
       "     'answers': [{'text': 'Vamsy', 'answer_start': 104}],\n",
       "     'id': '-107019484199702154',\n",
       "     'lang': 'te',\n",
       "     'split': 'train'}]}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xorqa_xx['train'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:24:55.270651Z",
     "iopub.status.busy": "2022-06-14T10:24:55.270063Z",
     "iopub.status.idle": "2022-06-14T10:24:55.275200Z",
     "shell.execute_reply": "2022-06-14T10:24:55.274493Z",
     "shell.execute_reply.started": "2022-06-14T10:24:55.270621Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xorqa_answer_str(context, qas):\n",
    "    context_qa_pairs = []\n",
    "    for qa in qas:\n",
    "        question = qa['question']\n",
    "        lang = qa['lang']\n",
    "        answer = qa['answers'][0]['text']\n",
    "        answer_start = qa['answers'][0]['answer_start']\n",
    "        context_qa_pairs.append((context, question, answer, answer_start, lang))\n",
    "    return context_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:25:00.018936Z",
     "iopub.status.busy": "2022-06-14T10:25:00.018652Z",
     "iopub.status.idle": "2022-06-14T10:25:05.739609Z",
     "shell.execute_reply": "2022-06-14T10:25:05.739134Z",
     "shell.execute_reply.started": "2022-06-14T10:25:00.018906Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xorqa_xx_dataset = defaultdict(lambda: { 'train': [], 'val': [] })\n",
    "\n",
    "for split_name in ['train', 'val']:\n",
    "    for i, item in enumerate(xorqa_xx[split_name]):\n",
    "        paragraphs = item['paragraphs']\n",
    "#         print('.' ,end='')\n",
    "        for j, paragraph in enumerate(paragraphs):\n",
    "\n",
    "            context = paragraph['context']\n",
    "            context_qa_pairs = get_xorqa_answer_str(context=context, qas=paragraph['qas'])\n",
    "\n",
    "            for context_qa_pair in context_qa_pairs:\n",
    "                context, question, answer, answer_start, lang = context_qa_pair\n",
    "                gt_sentence = mine_prompt_gt((context, question, answer, answer_start))\n",
    "                qa_item = {\n",
    "                     'question': question,\n",
    "                     'lang': lang,\n",
    "                     'context': context,\n",
    "                     'segmented_context': segment_sentence(context),\n",
    "                     'answer': answer,\n",
    "                     'answer_start': answer_start,\n",
    "                     'gt_sentence': gt_sentence,\n",
    "                }\n",
    "                xorqa_xx_dataset[lang][split_name].append(qa_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:25:05.740973Z",
     "iopub.status.busy": "2022-06-14T10:25:05.740761Z",
     "iopub.status.idle": "2022-06-14T10:25:05.746281Z",
     "shell.execute_reply": "2022-06-14T10:25:05.745399Z",
     "shell.execute_reply.started": "2022-06-14T10:25:05.740935Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bn', 'ja', 'ko', 'ru', 'fi', ' ar', 'te', 'ar']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(xorqa_xx_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:25:05.747384Z",
     "iopub.status.busy": "2022-06-14T10:25:05.747110Z",
     "iopub.status.idle": "2022-06-14T10:25:05.752360Z",
     "shell.execute_reply": "2022-06-14T10:25:05.751323Z",
     "shell.execute_reply.started": "2022-06-14T10:25:05.747357Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xorqa_xx_dataset['ar']['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute question-sentence similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Load mUSE_small (v3) model (as a baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-06-14T09:25:42.613822Z",
     "iopub.status.busy": "2022-06-14T09:25:42.613593Z",
     "iopub.status.idle": "2022-06-14T09:32:56.562260Z",
     "shell.execute_reply": "2022-06-14T09:32:56.561136Z",
     "shell.execute_reply.started": "2022-06-14T09:25:42.613798Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f0/cf4h984532z6vpmlrm7rzb2c0000gn/T/ipykernel_48774/2845263979.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmuse_small_v3_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/project/vistec-AI/scads/CL_ReLKT/CL-ReLKT/exploratory/online_prompt/notebooks/exploring_sentence_level.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected a string, got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   is_hub_module_v1 = tf.io.gfile.exists(\n\u001b[1;32m     94\u001b[0m       native_module.get_module_proto_path(module_path))\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(handle)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mModule\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow_hub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     return resolver.atomic_download(handle, download, module_dir,\n\u001b[0;32m---> 68\u001b[0;31m                                     self._lock_file_timeout_sec())\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_lock_file_timeout_sec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36matomic_download\u001b[0;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m       \u001b[0;31m# Wait for lock file to disappear.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m       \u001b[0m_wait_for_lock_to_disappear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock_file_timeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m       \u001b[0;31m# At this point we either deleted a lock or a lock got removed by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m       \u001b[0;31m# owner or another process. Perform one more iteration of the while-loop,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36m_wait_for_lock_to_disappear\u001b[0;34m(handle, lock_file, lock_file_timeout_sec)\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "muse_small_v3_model = load_model('https://tfhub.dev/google/universal-sentence-encoder-multilingual/3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Load teacher models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-14T09:32:56.562723Z",
     "iopub.status.idle": "2022-06-14T09:32:56.562967Z",
     "shell.execute_reply": "2022-06-14T09:32:56.562842Z",
     "shell.execute_reply.started": "2022-06-14T09:32:56.562829Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "XQUAD_TEACHER_DIR = '../../../CL-ReLKT_store/models/XQUAD/teacher_model/'\n",
    "MLQA_TEACHER_DIR = '../../../CL-ReLKT_store/models/MLQA/teacher_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xquad_teacher_model = load_model(XQUAD_TEACHER_DIR)\n",
    "mlqa_teacher_model = load_model(MLQA_TEACHER_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Load student models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XQUAD_STUDENT_SUPPORTED_LANGS_DIR = '../../../CL-ReLKT_store/models/XQUAD/student_best_supported_languages/'\n",
    "XQUAD_STUDENT_UNSUPPORTED_LANGS_DIR = '../../../CL-ReLKT_store/models/XQUAD/student_best_unsupported_languages/'\n",
    "\n",
    "XORQA_STUDENT_SUPPORTED_LANGS_DIR = '../../../CL-ReLKT_store/models/XORQA/student_best_supported_languages/'\n",
    "XORQA_STUDENT_UNSUPPORTED_LANGS_DIR = '../../../CL-ReLKT_store/models/XORQA/student_best_unsupported_languages/'\n",
    "\n",
    "MLQA_STUDENT_SUPPORTED_LANGS_DIR = '../../../CL-ReLKT_store/models/MLQA/student_best_supported_languages/'\n",
    "MLQA_STUDENT_UNSUPPORTED_LANGS_DIR = '../../../CL-ReLKT_store/models/MLQA/student_best_unsupported_languages/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xquad_student_supported_langs_model = load_model(XQUAD_STUDENT_SUPPORTED_LANGS_DIR)\n",
    "xorqa_student_supported_langs_model = load_model(XORQA_STUDENT_SUPPORTED_LANGS_DIR)\n",
    "mlqa_student_supported_langs_model = load_model(MLQA_STUDENT_SUPPORTED_LANGS_DIR)\n",
    "\n",
    "xquad_student_unsupported_langs_model = load_model(XQUAD_STUDENT_UNSUPPORTED_LANGS_DIR)\n",
    "xorqa_student_unsupported_langs_model = load_model(XORQA_STUDENT_UNSUPPORTED_LANGS_DIR)\n",
    "mlqa_student_unsupported_langs_model = load_model(MLQA_STUDENT_UNSUPPORTED_LANGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MAPPING = {\n",
    "  # mUSE_small\n",
    "  'model-muse_small_v3': muse_small_v3_model,\n",
    "  # teacher    \n",
    "  'model-xquad_teacher': xquad_teacher_model,\n",
    "  'model-mlqa_teacher': mlqa_teacher_model,\n",
    "  # student\n",
    "  'model-xquad_student_supported_langs': xquad_student_supported_langs_model,\n",
    "  'model-xorqa_student_supported_langs': xorqa_student_supported_langs_model,\n",
    "  'model-mlqa_student_supported_langs': mlqa_student_supported_langs_model,\n",
    "  'model-xquad_student_unsupported_langs': xquad_student_unsupported_langs_model,\n",
    "  'model-xorqa_student_unsupported_langs': xorqa_student_unsupported_langs_model,\n",
    "  'model-mlqa_student_unsupported_langs': mlqa_student_unsupported_langs_model,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:25:13.125532Z",
     "iopub.status.busy": "2022-06-14T10:25:13.125261Z",
     "iopub.status.idle": "2022-06-14T10:25:13.131586Z",
     "shell.execute_reply": "2022-06-14T10:25:13.130744Z",
     "shell.execute_reply.started": "2022-06-14T10:25:13.125504Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dataset-xorqa_bn_train', 'dataset-xorqa_bn_val', 'dataset-xorqa_ja_train', 'dataset-xorqa_ja_val', 'dataset-xorqa_ko_train', 'dataset-xorqa_ko_val', 'dataset-xorqa_ru_train', 'dataset-xorqa_ru_val', 'dataset-xorqa_fi_train', 'dataset-xorqa_fi_val', 'dataset-xorqa_ar_train', 'dataset-xorqa_te_train', 'dataset-xorqa_te_val', 'dataset-xorqa_ar_val'])\n"
     ]
    }
   ],
   "source": [
    "DATASET_MAPPING = {}\n",
    "\n",
    "for lang in list(xorqa_xx_dataset.keys()):\n",
    "    if len(xorqa_xx_dataset[lang]['train']) != 0:\n",
    "        DATASET_MAPPING[f'dataset-xorqa_{lang.strip()}_train'] = xorqa_xx_dataset[lang]['train']\n",
    "    if len(xorqa_xx_dataset[lang]['val']) != 0:\n",
    "        DATASET_MAPPING[f'dataset-xorqa_{lang.strip()}_val'] = xorqa_xx_dataset[lang]['val']\n",
    "print(DATASET_MAPPING.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Run inference and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function `run_online_prompt_mining` iterates over question-answer-passage triplets $(q_i, a_i, p_i)$ and compute \n",
    "the cosine similarity scores between question $q_i$ and segmented setences $s^i_j \\textrm{ where } p_i = ( s^i_0, \\ldots , s^i_{|p_i| - 1} )$ , and rank each quesiton-sentence pair by similairy score. Then, it evaluate the sentence-level precision@k.  Note: There is only 1 groundtruth sentence (i.e. the sentence where the answer span is a part of). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset_prefix, dataset in DATASET_MAPPING.items():\n",
    "    print(f'\\n\\ndataset_prefix: {dataset_prefix}')\n",
    "    for model_prefix, model in MODEL_MAPPING.items():\n",
    "        \n",
    "        print(f'\\n - model_prefix: {model_prefix}')\n",
    "        prefix = f'{dataset_prefix}+{model_prefix}'\n",
    "        _result = run_online_prompt_mining(dataset,\n",
    "                             prefix=f'{dataset_prefix}_{model_prefix}',\n",
    "                             model=model)\n",
    "\n",
    "\n",
    "        results[dataset_prefix][model_prefix] = _result\n",
    "        print('--'*50)\n",
    "    print('\\n')    \n",
    "    print('=='*50)\n",
    "    print('\\n')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Write result as JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results, open('./eval_results.dataset_name-xorqa.json', 'w'), ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert evaluation results to a pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = json.load(open('./eval_results.dataset_name-xorqa.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results.keys()), len(list(results.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_objs = []\n",
    "for dataset_name, result_model_group in data_teacher.items():\n",
    "    for model_name, (metric, raw_result) in result_model_group.items():\n",
    "        top1, precision_at_k = metric\n",
    "        \n",
    "        result_objs.append({\n",
    "            'dataset_name': dataset_name,\n",
    "            'model_name': model_name,\n",
    "            'precision_at_1': top1,\n",
    "            'precision_at_2': precision_at_k['2'],\n",
    "            'precision_at_3': precision_at_k['6'],\n",
    "            'precision_at_4': precision_at_k['4'],\n",
    "            'precision_at_5': precision_at_k['5'],\n",
    "            'precision_at_10': precision_at_k['10'],\n",
    "        })\n",
    "    \n",
    "df = pd.DataFrame.from_dict(result_obj)\n",
    "df.to_csv('./eval_results.dataset_name-xorqa.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
